\section{Background}\label{sec:background}
This section presents an overview of the CMS's discussed in this report and briefly examines the language constructs used within each system. Additionally, as \texttt{mgmt} is a new system which challenges the architectural model of established configuration management systems, this section presents a brief overview of \texttt{mgmt}. Lastly, migration possibilities for the selected systems are discussed.

\subsection{Product overview}
In this report we repeatedly discuss configuration management systems. As this term carries a somewhat ambiguous definition, we settle on the definition as presented by Lexis \cite{lexis_2016} during a guest lecture at the University of Amsterdam. He defines configuration management as \textit{"the techniques and policies to track hardware, infrastructure and software, and the configuration thereof"}. During the course of this report we will use this definition exclusively.

There are a lot of CMS's on the market and commonly each system employs a different architectural model. Our selection, consisting of Puppet, Ansible and \texttt{mgmt} each use a different architectural model. Additionally, each system uses system-specific language constructs. To provide some clarity prior to going into a migration, the main differences between each system are visualized in table \ref{table:comparison}. As shown in table \ref{table:comparison} there are various different architectures for configuration management systems available. Each having their own advantages and disadvantages. The products in the table are introduced years apart from each other and exhibit a clear trend. Products went from pull-based to push-based systems. Nowadays efforts are being made to create a distributed model.

\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
 & \textbf{Puppet} & \textbf{Ansible} & \textbf{\texttt{Mgmt}} \\ \midrule
\textbf{Introduced} & 2005 & 2012 & 2016 \\
\textbf{Architecture} & Pull & Push & Distributed \\
\textbf{Client} & Yes & No & Mesh of agents \\
\textbf{TCP port} & 8140 & 22 & 2379 \\
\textbf{Language} & Puppet DSL & YAML & Experimental DSL (YAML) \\
\textbf{Processing} & (Semi) Random execution & Sequential execution & Parallel execution \\
\textbf{Event driven} & No & No & Yes \\
\textbf{OS support} & \begin{tabular}[c]{@{}l@{}}Major operating systems \\ that allow a Puppet Agent\end{tabular} & Every OS with support for SSH & \begin{tabular}[c]{@{}l@{}}UNIX based systems with \\ \texttt{systemd} and PackageKit \\ support\end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{Comparison between most used configuration management systems a new model}
\label{table:comparison}
\end{table}
\noindent
All configuration management systems employ a notion of idempotence. An idempotent operation is one which can be applied multiple times without causing the result to diverge from the desired state \cite{shubin2016}. The following sections explain for each system and model how this desired state is achieved and the advantages and disadvantages of the employed models are discussed.

\subsection{Architectural models}\label{subsec:archmod}
Traditionally deployed CMS's generally use either a push- or a pull-based system to inform clients about configuration state updates \cite{papazoglou2003service}. Puppet and Chef are pull-based CMS's. This means that an agent program needs to be installed on the managed machine. This client then periodically checks in to the server. For Puppet the default time is 30 minutes. So every 30 minutes the configuration as it should be according to Puppet is compared to the current configuration of the server. The Puppet configuration can make sure a specific version of Apache is installed or whether a website contains a certain line of text for example. When the configuration differs from the configuration set in the Puppet configuration it will be changed immediately. Ansible and Salt on the other hand use a push-based technique to send commands to a managed machine. Ansible uses 'playbooks' which hold the command set for specific servers and divide machines in groups which determine the destination for a command set. The management server starts communication over SSH to the management machines and delivers and runs a Python script on the client. This means that the capability to execute Python code and an SSH-server are prerequisites for Ansible. The Python script is removed right after it is executed. This way of configuring servers is on demand. Like Puppet, Ansible is able to check for specific software versions or check file content and change it if needed. When a configuration differs from the configuration in the playbook the client will be changed when the playbook is run by an administrator. So when a playbook is not run, the configuration could differ from the intended configuration for a long time. By default Ansible does not peform periodic configuration pushes. A visual representation of the used architectural models is shown in figure \ref{fig:archmodel}. 

\begin{figure}[H]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.5]{img/pull.png}
  \caption{Pull architecture}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.5]{img/push.png}
  \caption{Push architecture}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.37]{img/distributed.png}
  \caption{Distributed architecture}
  \label{fig:sub3}
\end{subfigure}
\caption{Architectural models employed by configuration management systems}
\label{fig:archmodel}
\end{figure}
\noindent
Using a push-based model which relies on SSH opens up opportunities to manage a larger set of machines. When a CMS only needs SSH to remotely configure a machine, the range of systems it could manage is a lot larger than an agent based system. If the system to manage has a closed operating system it can't be managed by a pull-based CMS. All large networking brands make sure their operating systems (OS) are able to keep running for years without interruption. To make sure the system stays up the OS is closed down making sure no third party applications are able to disrupt the system. Push-based systems only rely on a SSH connection. Ansible is capable of executing the Python script on the server, therefore the requirement of Python on the managed server is no longer there. This makes it possible to manage closed source network operating systems like Juniper or Cisco devices. 

A router or switch could be set up with the necessary basics to make sure it can be placed at the designated location and is accessible over the network by SSH. Ansible can be set up to do the rest of the configuration. Running a playbook that will generate a base configuration for all the machines within the same class and check the running configuration to see if it matches. Besides configuration changes depending on the brand of equipment, Ansible is capable of performing a code upgrade on network devices. Some of the new devices offer nonstop service upgrades \cite{NSSU}\cite{ISSU}, which actually means that upgrading the system during office hours without any downtime (when everything is configured in a proper way and all requirements are matched) becomes a possibility. When the installed base of a specific brand and type is big enough automation of configuration or code changes have as much advantages on network devices as it has on server configuration.

\subsection{Distributed configuration management}\label{subsec:distributedmgmt}
As previously alluded to, a relatively new configuration management system on the horizon is \texttt{mgmt}. \texttt{Mgmt} is currently classified as a 'research prototype next generation configuration management system' and was initially developed by Shubin \cite{shubin2016} as a series of extensions to Puppet. The system focuses on key points in which current CMS are currently lacking, namely \textit{parallelization}, \textit{event driven convergence} and most notably, a \textit{distributed architecture} \footnote{The source code for \texttt{mgmt} is available on GitHub: \url{https://github.com/purpleidea/mgmt/}}. 

\subsubsection{Distribution}
The main point of innovation in \texttt{mgmt} is the distributed topology of the system. As discussed in section \ref{subsec:archmod}, more traditional systems like Puppet and Ansible generally run in a client-server, push or pull manner. Naturally with these models, as all code is placed in a central location, scalability and performance issues arise, especially when the amount of clients increases. A logical step moving forward would be to distribute the codebase over all machines under control of the management system. 

In order to achieve distribution \texttt{mgmt} relies on the Raft \cite{ongaro_2016} consensus algorithm to create a (full) mesh of \texttt{mgmt} agents as shown in figure \ref{fig:sub3}. This algorithm fits the theme of the CMS as it attempts to be an easier to understand version of Paxos. All resources as defined in the DSL are stored as a key-value pair in an \texttt{etcd} data store and due to the full mesh, every node can add or remove resources from the store. Due to the distribution of the key-value pairs, a failing \texttt{etcd} instance can be overcome by communicating with a different instance in the mesh. 

As with any CMS however, scalability remains an issue. Creating a full mesh of connections is expensive as it requires $n(n-1)/2$ connections where $n$ is the amount of nodes in the mesh. As the total amount of nodes in the mesh increases, the total amount of required interconnections grows exponentially. Additionally, replicating the data between all key-value stores becomes more intensive each time a node is added. In order to overcome these issues \texttt{mgmt} elects a series of nodes to become \texttt{etcd} masters, which host an instance of the key-value store. All other nodes connect to these instances. As such a form of hierarchy is created in \texttt{mgmt}. This would add another layer around the distributed mesh as shown in figure \ref{fig:sub3}.

The concept of sharing resources between nodes is not new however. Puppet has implemented this in its DSL language by means of an 'Exported Resource' \cite{exported_2016}. These resources allow the Puppet compiler to share information among nodes by combining information from multiple nodesâ€™ catalogs. The downside of these exported resources is that it is an expensive process to generate a consistent view of the resources among all Puppet Masters in the cluster. In order to share the resource, the Puppet master sends a copy of every catalog it compiles to the central PuppetDB. The PuppetDB at that point sequentially runs through every resource definition and retains the most recent catalog for every node. Subsequently the compiled catalog is provided back to all Puppet Masters in the cluster. By using a shared key-value store \texttt{mgmt} attempts to alleviate this problem. 

Sharing the codebase does however impose security risks. Every managed node (hosting a key-value store) at this point contains all state definitions for all possible nodes in the IT environment. Push- or pull-based systems don't share this security risk as specific code is delegated to specific machines. It remains to be seen how \texttt{mgmt} will deal with this issue in the future. At this point in time no fine-grained segregation mechanisms are in place. 

\subsubsection{Parallelization}
Besides distribution \texttt{mgmt} introduces the ability to perform actions in parallel. As displayed in table \ref{table:comparison}, traditional CMS's perform their actions in a sequential order. By using a parallel mode of operation, actions can be performed faster thus a desired state can be reached quicker. However, providing the option for parallel execution introduces room for errors as well. In a simple example, running two package installations with \texttt{apt} in parallel would fail due to a global lock on \texttt{dpkg}. Similarly, dependencies between applications and libraries would have to be resolved in a sequential manner. \texttt{Mgmt} deals with this scenario by batching (or grouping) all blocking operations and putting them in a sequential order. All operations which can be parallelized are placed in disjoint graphs. 

Parallelization does allow a system to reach its end state more rapidly. Long running processes which limited the deployment speed in a sequential process can now be converge quicker. In a real world scenario it isn't uncommon for a company to have multiple disconnected modules without inter-dependencies running simultaneously. The experiments in section \ref{sec:methodology} further explore the concept of parallelization. 

\subsubsection{Event based model}
Traditional CMS's are limited by the fact that a desired state is only applied perodically or manually. This mode of operation allows a configuration to diverge during the timeframe in which no check operation is performed by the master server. Naturally, a more preferable method would be to define an end state and have it applied constantly. \texttt{Mgmt} attempts to achieve his by triggering a reconvergence operation based on system events. For example, when a file is changed on the system which is being managed by \texttt{mgmt}, an \texttt{inotify} event is triggered (which is coupled to \texttt{dbus}. Based on this event the \texttt{mgmt} agent checks the desired state based on key-value pairs in the distributed \texttt{etcd} store and reconverges to that given state. Similarly for service states \texttt{systemd} events are checked and software packages are based on PackageKit events. By utilising this methodology, rapid convergence of the desired state in an environment can be achieved. The event based nature of \texttt{mgmt} is further explored in a separate experiment in section \ref{sec:methodology}. As shown in table \ref{table:comparison}, the dependency on these event triggers limits \texttt{mgmt} to UNIX systems which use PackageKit and \texttt{systemd}.  

\subsection{Language constructs}
A primary reason for companies to migrate away from Puppet is because defining a desired state in Puppets proprietary DSL is relatively difficult. Puppet uses modules in which the defined resources are hierachically structured. The hierarhical structure is achieved by defining a series of interconnected class definitions. When modules grow in size and become more complex the amount of code required to define a state becomes unmanageable. Lane \cite{movingawayfrompuppet} from the company 'Lyft' acknowledges this issue and talks about how their Puppet code base consisted of approximately ten thousand lines of code where the same result was achieved by circa a thousand lines of Ansible code after migrating. Ansible on the other hand uses a DSL based on Yet Another Markup Language (YAML) which allows for more human readable code with actions being defined in a sequential manner. Listing \ref{lst:shellshock} provides an example of a piece of Ansible code which can fix the Shellshock vulnerability in a relatively small amount of lines \cite{severdensity_2015}. The Puppet module counterpart on the other hand consists of many hierachically structured and interconnected class definitions \footnote{A Puppet module for fixing the Shellshock vulnerability is available at \url{https://forge.puppetlabs.com/renanvicente/shellshock/readme}.}.
\\
\begin{lstlisting}[caption={Shellshock security patching with Ansible playbook},label=lst:shellshock]
- hosts: all
  gather_facts: yes
  remote_user: administrator
  sudo: yes
  tasks:
    - name: Update Shellshock (Debian)
      apt: name=bash
           state=latest
           update_cache=yes
      when: ansible_os_family == "Debian"

    - name: Update Shellshock (RedHat)
      yum: name=bash
           state=latest
           update_cache=yes
      when: ansible_os_family == "RedHat"
\end{lstlisting}
\noindent
\texttt{Mgmt}, like Ansible, relies on similar YAML-based language constructs. This system uses 'Resource Graph Definitions' which consist of sequentially defined resources, similarly like in Ansible. Listing \ref{lst:resgraph} presents an example of a truncated \texttt{mgmt} state definition. A full example of a resource graph in \texttt{mgmt} can be found in Appendix \ref{app:mgmtgraph} and is discussed in more detail in section \ref{sec:methodology}.
\\
\begin{lstlisting}[caption={Snippet of Resource Graph Definition},label=lst:resgraph]
---
graph: apache2
comment: Parallel installation of Apache2 (Proof of Concept)
resources:
[...]  # Output omitted
  pkg:
  - name: apache2
    state: installed
  svc:
  - name: apache2
    meta:
      autoedge: false
    state: started
\end{lstlisting}
\noindent
By using a similar language like Ansible, \texttt{mgmt} would be a logical migration target in the near future coming from Ansible. Converting Puppet modules to \texttt{mgmt} graphs on the other hand is a rather complex process as displayed by Frank \cite{frank_2016}. He uses a series of regular expressions to convert a Puppet module to a Resource Graph. Currently his methodology struggles when the Puppet module is defined hierachically.

\subsection{Migration strategies}\label{subsec:migrationstrategies}
Migrating between systems can be done by using various strategies and with a broad range of systems. When using CMS A on all of the servers, new servers can be managed by CMS B. This will result in a slow migration between the management systems. The lifetime of a server dictates the time it will take to migrate, nowadays only the low level systems like NTP, DNS and authentication server should be bare metal servers. All other services depending on these could be virtual. The lifetime of these virtual servers is often less than the lifetime of bare metal machines because the virtual machines are created to support a specific service and are terminated directly when the service is no longer needed. Some special cases are excluded from this example, for instance a DHCP servers. When using virtual machines the migration process can be finished fast. In a cluster of 10 web servers managed by CMS A offering the same website, a new webserver could be created managed by CMS B. When tested and approved it can be added to the cluster. An old server should be removed from the cluster and decommissioned. In Large scale environments this will be an extensive job. Try doing this in a farm of a hundred web front-end servers.

\begin{quote}
According to Jaap van Ginkel on thursday 10-3-2016 during a lecture on Large Installation Administration, "More system administrators scale out services by using virtual machines"
\end{quote}

To keep server management easy and effective. One configuration management system should be used and the migration should not stretch multiple years. A Big bang scenario is a proper alternative. Migrating all the webservers from CMS A to CMS B by a set of automated steps without installing new servers. Therefore not using extra resources and migrating without downtime. This big bang scenario \cite{bigbang} is explained in this report as the focus of this report lays on Large Installation Administration.
